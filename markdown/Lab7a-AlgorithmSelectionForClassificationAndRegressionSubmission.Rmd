---
title: "Business Intelligence Lab Submission Markdown"
author: "<team kubwa>"
date: "<23/10/2023>"
output:
  github_document: 
    toc: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    df_print: default
editor_options:
  chunk_output_type: console
---

# Student Details

+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Student ID Numbers and Names of Group Members** | *\<list one Student name, class group (just the letter; A, B, or C), and ID per line, e.g., 123456 - A - John Leposo; you should be between 2 and 5 members per group\>* |
|                                                   |                                                                                                                                                                          |
|                                                   | 1.  128998 - B - Crispus Nzano                                                                                                                                            |
|                                                   |                                                                                                                                                                          |
|                                                   | 2.  134100 - B - Timothy Obosi                                                                                                                                             |
|                                                   |                                                                                                                                                                          |
|                                                   | 3.  134092 - B - Alison Kuria                                                                                                                                                                                                                                                 |
|                                                   |                                                                                                                                                                          |
|                                                   | 4.  135269 - B - Clifford Kipchumba                                                                                                                          |
|                                                   |                                                                                                                                                      |
|                                                   | 5.  112826 - B - Matu Ngatia                                                                                                                          |
|                                                   |                                                  
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **GitHub Classroom Group Name**                   | Team Kubwa                                                                                                       |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Course Code**                                   | BBT4206                                                                                                                                                                  |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Course Name**                                   | Business Intelligence II                                                                                                                                                 |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Program**                                       | Bachelor of Business Information Technology                                                                                                                              |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| **Semester Duration**                             | 21^st^ August 2023 to 28^th^ November 2023                                                                                                                               |
+---------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

# Setup Chunk

We start by installing all the required packages

```{r Install Packages, echo=TRUE, message=FALSE, warning=FALSE}
## formatR - Required to format R code in the markdown ----

if (require("languageserver")) {
  require("languageserver")
} else {
  install.packages("languageserver", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

# Introduction ----
# Resampling methods are techniques that can be used to improve the performance
# and reliability of machine learning algorithms. They work by creating
# multiple training sets from the original training set. The model is then
# trained on each training set, and the results are averaged. This helps to
# reduce overfitting and improve the model's generalization performance.

# Resampling methods include:
## Splitting the dataset into train and test sets ----
## Bootstrapping (sampling with replacement) ----
## Basic k-fold cross validation ----
## Repeated cross validation ----
## Leave One Out Cross-Validation (LOOCV) ----

# STEP 1. Install and Load the Required Packages ----
## stats ----
if (require("stats")) {
  require("stats")
} else {
  install.packages("stats", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## mlbench ----
if (require("mlbench")) {
  require("mlbench")
} else {
  install.packages("mlbench", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## caret ----
if (require("caret")) {
  require("caret")
} else {
  install.packages("caret", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## MASS ----
if (require("MASS")) {
  require("MASS")
} else {
  install.packages("MASS", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## glmnet ----
if (require("glmnet")) {
  require("glmnet")
} else {
  install.packages("glmnet", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## e1071 ----
if (require("e1071")) {
  require("e1071")
} else {
  install.packages("e1071", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## kernlab ----
if (require("kernlab")) {
  require("kernlab")
} else {
  install.packages("kernlab", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

## rpart ----
if (require("rpart")) {
  require("rpart")
} else {
  install.packages("rpart", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}


```

------------------------------------------------------------------------

**Note:** the following "*KnitR*" options have been set as the defaults in this markdown:\
`knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, collapse = FALSE, tidy.opts = list(width.cutoff = 80), tidy = TRUE)`.

More KnitR options are documented here <https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html> and here <https://yihui.org/knitr/options/>.

```{r setup, echo=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
	eval = TRUE,
	echo = TRUE,
	warning = FALSE,
	collapse = FALSE,
	tidy = TRUE
)
```

------------------------------------------------------------------------

**Note:** the following "*R Markdown*" options have been set as the defaults in this markdown:

> output:\
> \
> github_document:\
> toc: yes\
> toc_depth: 4\
> fig_width: 6\
> fig_height: 4\
> df_print: default\
> \
> editor_options:\
> chunk_output_type: console

# Loading the Loan Status Train Imputed Dataset

The Datasets are then loaded. 

```{r Load Datasets}

library(readr)
cubic_zirconia <- read_csv("C:/Users/Cris/github-classroom/BBT4206-R-Lab7of15-AlgorithmSelection-team-kubwa/data/cubic_zirconia.csv")
View(cubic_zirconia)


library(readr)
train_imputed <- read_csv("C:/Users/Cris/github-classroom/BBT4206-R-Lab7of15-AlgorithmSelection-team-kubwa/data/train_imputed.csv")
View(train_imputed)


```

## Description of the Dataset

We then display the number of observations and number of variables. 12 Variables and 614 observations.

```{r Your Fourth Code Chunk}
dim(train_imputed)
dim(cubic_zirconia)
```

Next, we display the quartiles for each numeric variable[*... this is the process of **"storytelling using the data."** The goal is to analyse the Loan and Cubic dataset and try to train a model to make predictions( which model is most suited for this dataset).*]{#highlight style="color: blue"}

```{r Your Fifth Code Chunk}
summary(train_imputed)
summary(cubic_zirconia)
```

# \<Linear Regression\>

Linear Regression using Ordinary Least Squares without caret. The lm() function is in the stats package and creates a linear regression model using ordinary least squares (OLS).

```{r Your Sixth Code Chunk}
# Define an 80:20 train:test data split of the cubic_zirconia dataset.
train_index <- createDataPartition(cubic_zirconia$price,
                                   p = 0.8,
                                   list = FALSE)
cubic_zirconia_train <- cubic_zirconia[train_index, ]
cubic_zirconia_test <- cubic_zirconia[-train_index, ]

#### Train the model ----
cubic_zirconia_model_lm <- lm(price ~ ., cubic_zirconia_train)

#### Display the model's details ----
print(cubic_zirconia_model_lm)

#### Make predictions ----
predictions <- predict(cubic_zirconia_model_lm, cubic_zirconia_test[, 1:10])
print(predictions)
#### Display the model's evaluation metrics ----
##### RMSE ----
rmse <- sqrt(mean((cubic_zirconia_test$price - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
# SSR is the sum of squared residuals (the sum of squared differences
# between observed and predicted values)
ssr <- sum((cubic_zirconia_test$price - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
# SST is the total sum of squares (the sum of squared differences
# between observed values and their mean)
sst <- sum((cubic_zirconia_test$price - mean(cubic_zirconia_test$price))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
# We then use SSR and SST to compute the value of R squared.
# The closer the R squared value is to 1, the better the model.
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
# MAE is expressed in the same units as the target variable, making it easy to
# interpret. For example, if you are predicting the amount paid in rent,
# and the MAE is KES. 10,000, it means, on average, your model's predictions
# are off by about KES. 10,000.
absolute_errors <- abs(predictions - cubic_zirconia_test$price)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))



```

## \<1.b. Linear Regression using Ordinary Least Squares with caret\>
Load and split the dataset

```{r Your Seventh Code Chunk}
# Define an 80:20 train:test data split of the dataset.
train_index <- createDataPartition(cubic_zirconia$price,
                                   p = 0.8,
                                   list = FALSE)
cubic_zirconia_train <- cubic_zirconia[train_index, ]
cubic_zirconia_test <- cubic_zirconia[-train_index, ]

#### Train the model ----
set.seed(7)
train_control <- trainControl(method = "cv", number = 5)
cubic_zirconia_caret_model_lm <- train(price ~ ., data = cubic_zirconia_train,
                                       method = "lm", metric = "RMSE",
                                       preProcess = c("center", "scale"),
                                       trControl = train_control)

#### Display the model's details ----
print(cubic_zirconia_caret_model_lm)

#### Make predictions ----
predictions <- predict(cubic_zirconia_caret_model_lm,
                       cubic_zirconia_test[, 1:10])

#### Display the model's evaluation metrics ----
##### RMSE ----
rmse <- sqrt(mean((cubic_zirconia_test$price - predictions)^2))
print(paste("RMSE =", sprintf(rmse, fmt = "%#.4f")))

##### SSR ----
# SSR is the sum of squared residuals (the sum of squared differences
# between observed and predicted values)
ssr <- sum((cubic_zirconia_test$price - predictions)^2)
print(paste("SSR =", sprintf(ssr, fmt = "%#.4f")))

##### SST ----
# SST is the total sum of squares (the sum of squared differences
# between observed values and their mean)
sst <- sum((cubic_zirconia_test$price - mean(cubic_zirconia_test$price))^2)
print(paste("SST =", sprintf(sst, fmt = "%#.4f")))

##### R Squared ----
# We then use SSR and SST to compute the value of R squared.
# The closer the R squared value is to 1, the better the model.
r_squared <- 1 - (ssr / sst)
print(paste("R Squared =", sprintf(r_squared, fmt = "%#.4f")))

##### MAE ----
# MAE is expressed in the same units as the target variable, making it easy to
# interpret. For example, if you are predicting the amount paid in rent,
# and the MAE is KES. 10,000, it means, on average, your model's predictions
# are off by about KES. 10,000.
absolute_errors <- abs(predictions - cubic_zirconia_test$price)
mae <- mean(absolute_errors)
print(paste("MAE =", sprintf(mae, fmt = "%#.4f")))

```

## \<Logistic Regression with caret\>
The glm() function is in the stats package and creates a generalized linear model for regression or classification. It can be configured to perform a logistic regression suitable for binary classification problems

```{r Your Eighth Code Chunk}
### 2.b. Logistic Regression with caret ----
#### Load and split the dataset ----

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(train_imputed$Status,
                                   p = 0.7,
                                   list = FALSE)
train_imputed_train <- train_imputed[train_index, ]
train_imputed_test <- train_imputed[-train_index, ]

#### Train the model ----
# We apply the 5-fold cross validation resampling method
train_control <- trainControl(method = "cv", number = 5)
# We can use "regLogistic" instead of "glm"
# Notice the data transformation applied when we call the train function
# in caret, i.e., a standardize data transform (centre + scale)
set.seed(7)
train_imputed_model_logistic <-
  train(Status ~ ., data = train_imputed_train,
        method = "regLogistic", metric = "Accuracy",
        preProcess = c("center", "scale"), trControl = train_control)

#### Display the model's details ----
print(train_imputed_model_logistic)

#### Make predictions ----
predictions <- predict(train_imputed_model_logistic,
                       train_imputed_test[, 1:11])
print(predictions)
#### Display the model's evaluation metrics ----

levels(predictions)
levels(train_imputed_test[, 1:12]$Status)

# Define the levels you expect in the Status variable
expected_levels <- c("Y", "N")

# Convert the Status variable to a factor with the defined levels
train_imputed_test[, 1:12]$Status <- factor(train_imputed_test[, 1:12]$Status, levels = expected_levels)

# Assuming predictions is a character vector, convert it to a factor
#predictions <- factor(predictions, levels = levels(train_imputed_test[, 1:12]$Status))

confusion_matrix <-
  caret::confusionMatrix(predictions,
                         train_imputed_test[, 1:12]$Status)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```

## \<Linear Discriminant Analysis\>
The lda() function is in the MASS package and creates a linear model of a multi-class classification problem.

```{r Your Ninth Code Chunk}
## 2.a. Load the dataset ----


# Define a 70:30 train:test data split of the Loan dataset.
train_index <- createDataPartition(train_imputed$Status,
                                   p = 0.7,
                                   list = FALSE)
train_imputed_train <- train_imputed[train_index, ]
train_imputed_test <- train_imputed[-train_index, ]

#### Train the model ----
train_imputed_model_lda <- lda(Status ~ ., data = train_imputed_train)

#### Display the model's details ----
print(train_imputed_model_lda)

#### Make predictions ----
predictions <- predict(train_imputed_model_lda,
                       train_imputed_test[, 1:11])$class

#### Display the model's evaluation metrics ----
table(predictions, train_imputed_test$Status)
```

## \<Regularized Linear Regression\>
The glmnet() function is in the glmnet package and can be used for both classification and regression problems. It can also be configured to perform three important types of regularization:
  1. lasso,
  2. ridge and
  3. elastic net
by configuring the alpha parameter to 1, 0 or in [0,1] respectively.

```{r Your Tenth Code Chunk}
###c. Regularized Linear Regression Classification Problem with CARET ----
#### Load and split the dataset ----

# Define a 70:30 train:test data split of the loan dataset.
train_index <- createDataPartition(train_imputed$Status,
                                   p = 0.7,
                                   list = FALSE)
train_imputed_train <- train_imputed[train_index, ]
train_imputed_test <- train_imputed[-train_index, ]

#### Train the model ----
# We apply the 5-fold cross validation resampling method
set.seed(7)
train_control <- trainControl(method = "cv", number = 5)
train_imputed_caret_model_glmnet <-
  train(Status ~ ., data = train_imputed_train,
        method = "glmnet", metric = "Accuracy",
        preProcess = c("center", "scale"), trControl = train_control)

#### Display the model's details ----
print(train_imputed_caret_model_glmnet)

#### Make predictions ----
predictions <- predict(train_imputed_caret_model_glmnet,
                       train_imputed_test[, 1:11])

#### Display the model's evaluation metrics ----

# Define the levels you expect in the Status variable
expected_levels <- c("Y", "N")

# Convert the Status variable to a factor with the defined levels
train_imputed_test[, 1:12]$Status <- factor(train_imputed_test[, 1:12]$Status, levels = expected_levels)


confusion_matrix <-
  caret::confusionMatrix(predictions,
                         train_imputed_test[, 1:12]$Status)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```

## \<Non-Linear Algorithms \>
1.  Classification and Regression Trees ----
1.a. Decision tree for a classification problem without caret
Load and split the dataset

```{r Your Eleventh Code Chunk}

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(train_imputed$Status,
                                   p = 0.7,
                                   list = FALSE)
train_imputed_train <- train_imputed[train_index, ]
train_imputed_test <- train_imputed[-train_index, ]

#### Train the model ----
train_imputed_model_rpart <- rpart(Status ~ ., data = train_imputed_train)

#### Display the model's details ----
print(train_imputed_model_rpart)

#### Make predictions ----
predictions <- predict(train_imputed_model_rpart,
                       train_imputed_test[, 1:11],
                       type = "class")

#### Display the model's evaluation metrics ----
table(predictions, train_imputed_test$Status)


# Define the levels you expect in the Status variable
expected_levels <- c("Y", "N")

# Convert the Status variable to a factor with the defined levels
train_imputed_test[, 1:12]$Status <- factor(train_imputed_test[, 1:12]$Status, levels = expected_levels)

confusion_matrix <-
  caret::confusionMatrix(predictions,
                         train_imputed_test[, 1:12]$Status)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```

## \<Non-Linear Algorithms \>
1.  Classification and Regression Trees ----
1.a. Decision tree for a classification problem without caret
Load and split the dataset

```{r Your Twelfth Code Chunk}

### 1.c. Decision tree for a classification problem with caret ----
#### Load and split the dataset ----

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(train_imputed$Status,
                                   p = 0.7,
                                   list = FALSE)
train_imputed_train <- train_imputed[train_index, ]
train_imputed_test <- train_imputed[-train_index, ]

#### Train the model ----
set.seed(7)
# We apply the 5-fold cross validation resampling method
train_control <- trainControl(method = "cv", number = 5)
train_imputed_caret_model_rpart <- train(Status ~ ., data = train_imputed,
                                    method = "rpart", metric = "Accuracy",
                                    trControl = train_control)

#### Display the model's details ----
print(train_imputed_caret_model_rpart)

#### Make predictions ----
predictions <- predict(train_imputed_model_rpart,
                       train_imputed_test[, 1:11],
                       type = "class")

#### Display the model's evaluation metrics ----
table(predictions, train_imputed_test$Status)

# Define the levels you expect in the Status variable
expected_levels <- c("Y", "N")

# Convert the Status variable to a factor with the defined levels
train_imputed_test[, 1:12]$Status <- factor(train_imputed_test[, 1:12]$Status, levels = expected_levels)

confusion_matrix <-
  caret::confusionMatrix(predictions,
                         train_imputed_test[, 1:12]$Status)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```
## \<Naïve Bayes \>
Naïve Bayes Classifier for a Classification Problem without CARET We use the naiveBayes function inside the e1071 package.

Load and split the dataset 

```{r Your Fifteenth Code Chunk}


# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(train_imputed$Status,
                                   p = 0.7,
                                   list = FALSE)
train_imputed_train <- train_imputed[train_index, ]
train_imputed_test <- train_imputed[-train_index, ]

#### Train the model ----
train_imputed_model_nb <- naiveBayes(Status ~ .,
                                data = train_imputed_train)

#### Display the model's details ----
print(train_imputed_model_nb)

#### Make predictions ----
predictions <- predict(train_imputed_model_nb,
                       train_imputed_test[, 1:11])

#### Display the model's evaluation metrics ----
# Define the levels you expect in the Status variable
expected_levels <- c("Y", "N")

# Convert the Status variable to a factor with the defined levels
train_imputed_test[, 1:12]$Status <- factor(train_imputed_test[, 1:12]$Status, levels = expected_levels)

confusion_matrix <-
  caret::confusionMatrix(predictions,
                         train_imputed_test[, 1:12]$Status)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```

## \< k-Nearest Neighbours \>
The knn3() function is in the caret package and does not create a model.Instead it makes predictions from the training dataset directly. It can be used for classification or regression.

```{r Your Sixteenth Code Chunk}

### 3.c. kNN for a classification problem with CARET's train function ----
#### Load and split the dataset ----

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(train_imputed$Status,
                                   p = 0.7,
                                   list = FALSE)
train_imputed_train <- train_imputed[train_index, ]
train_imputed_test <- train_imputed[-train_index, ]

#### Train the model ----
# We apply the 10-fold cross validation resampling method
# We also apply the standardize data transform
set.seed(7)
train_control <- trainControl(method = "cv", number = 10)
train_imputed_caret_model_knn <- train(Status ~ ., data = train_imputed,
                                  method = "knn", metric = "Accuracy",
                                  preProcess = c("center", "scale"),
                                  trControl = train_control)

#### Display the model's details ----
print(train_imputed_caret_model_knn)

#### Make predictions ----
predictions <- predict(train_imputed_caret_model_knn,
                       train_imputed_test[, 1:11])

#### Display the model's evaluation metrics ----

# Define the levels you expect in the Status variable
expected_levels <- c("Y", "N")

# Convert the Status variable to a factor with the defined levels
train_imputed_test[, 1:12]$Status <- factor(train_imputed_test[, 1:12]$Status, levels = expected_levels)

confusion_matrix <-
  caret::confusionMatrix(predictions,
                         train_imputed_test[, 1:12]$Status)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```

## \< Support Vector Machine \>
SVM Classifier for a classification problem without CARET

```{r Your Seventeenth Code Chunk}

### 4.c. SVM Classifier for a classification problem with CARET ----
# The SVM with Radial Basis kernel implementation can be used with caret for
# classification as follows:
#### Load and split the dataset ----

# Define a 70:30 train:test data split of the dataset.
train_index <- createDataPartition(train_imputed$Status,
                                   p = 0.7,
                                   list = FALSE)
train_imputed_train <- train_imputed[train_index, ]
train_imputed_test <- train_imputed[-train_index, ]

#### Train the model ----
set.seed(7)
train_control <- trainControl(method = "cv", number = 5)
train_imputed_caret_model_svm_radial <- 
  train(Status ~ ., data = train_imputed_train, method = "svmRadial",
        metric = "Accuracy", trControl = train_control)

#### Display the model's details ----
print(train_imputed_caret_model_svm_radial)

#### Make predictions ----
predictions <- predict(train_imputed_caret_model_svm_radial,
                       train_imputed_test[, 1:11])

#### Display the model's evaluation metrics ----
table(predictions, train_imputed_test$Status)

# Define the levels you expect in the Status variable
expected_levels <- c("Y", "N")

# Convert the Status variable to a factor with the defined levels
train_imputed_test[, 1:12]$Status <- factor(train_imputed_test[, 1:12]$Status, levels = expected_levels)

confusion_matrix <-
  caret::confusionMatrix(predictions,
                         train_imputed_test[, 1:12]$Status)
print(confusion_matrix)

fourfoldplot(as.table(confusion_matrix), color = c("grey", "lightblue"),
             main = "Confusion Matrix")

```
**etc.** as per the lab submission requirements. Be neat and communicate in a clear and logical manner.








